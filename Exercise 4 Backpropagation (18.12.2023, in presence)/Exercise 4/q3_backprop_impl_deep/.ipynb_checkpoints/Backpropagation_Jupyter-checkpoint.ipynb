{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b872dd",
   "metadata": {},
   "source": [
    "## Question 1 - Sofmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Compute the exponentials of each input element\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtracting np.max(x) for numerical stability\n",
    "    # Normalize the exponentials so that their sum equals 1\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431eebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_jacobian(x):\n",
    "    # Compute softmax values for each element in vector x\n",
    "    S = softmax(x)\n",
    "    # Initialize the Jacobian matrix with zeros\n",
    "    jacobian = np.zeros((len(x), len(x)))\n",
    "    \n",
    "    # Fill in the Jacobian matrix\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            if i == j:\n",
    "                # Diagonal entries\n",
    "                jacobian[i, j] = S[i] * (1 - S[i])\n",
    "            else:\n",
    "                # Off-diagonal entries\n",
    "                jacobian[i, j] = -S[i] * S[j]\n",
    "    return jacobian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_jacobian_fast(x):\n",
    "    # Compute softmax values for each element in vector x\n",
    "    S = softmax(x)\n",
    "    # Compute the outer product of S with itself\n",
    "    outer_product = np.outer(S, S)\n",
    "    # Fill in the Jacobian matrix\n",
    "    jacobian = np.diag(S) - outer_product\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dca840-3bfe-4592-8f5c-aa9a4d23a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax_jacobian():\n",
    "    x = np.array([1, 2, 3])\n",
    "    jacobian = softmax_jacobian(x)\n",
    "    jacobian_fast = softmax_jacobian_fast(x)\n",
    "    assert np.allclose(jacobian, jacobian_fast) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf938819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_bprop(v,x):\n",
    "    #compute the softmax of x\n",
    "    sigma_x = softmax(x)\n",
    "\n",
    "    # Compute the dot produt of v and sigma_x^T\n",
    "    v_dot_sigmaT = np.dot(v, sigma_x)\n",
    "\n",
    "    # Compute z using the efficient formula\n",
    "    z = sigma_x * (v - v_dot_sigmaT)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cecb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_cross_entropy_loss(z,t):\n",
    "    # Compute the gradient of the cross-entropy loss with respect to z\n",
    "    grad = z - t\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c06e0",
   "metadata": {},
   "source": [
    "## Question 2 - Softmax-regression with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10834381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NNModule:\n",
    "    \"\"\" Class defining abstract interface every module has to implement\n",
    "\n",
    "    \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Forwardpropagate the input through the module\n",
    "\n",
    "        :param input: Input tensor for the module\n",
    "        :return Output tensor after module application\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def bprop(self, grad_out):\n",
    "        \"\"\" Backpropagate the gradient the output to the input\n",
    "\n",
    "        :param grad_out: Gradients at the output of the module\n",
    "        :return: Gradient wrt. input\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_grad_param(self, grad_out):\n",
    "        \"\"\" Return gradients wrt. the parameters\n",
    "        Calculate the gardients wrt. to the parameters of the module. Function already\n",
    "        accumulates gradients over the batch -> Save memory and implementation issues using numpy avoid loops\n",
    "\n",
    "        :param grad_out: Gradients at the output\n",
    "        :return: Gradients wrt. the internal parameter accumulated over the batch\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def apply_parameter_update(self, acc_grad_para, up_fun):\n",
    "        \"\"\" Apply the update function to the internal parameters.\n",
    "\n",
    "        :param acc_grad_para: Accumulated gradients over the batch\n",
    "        :param up_fun: Update function used\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    # If we would like to support different initialization techniques, we could\n",
    "    # use an Initializer class\n",
    "    # For simplicity use a fixed initialize for each module\n",
    "    @abc.abstractmethod\n",
    "    def initialize_parameter(self):\n",
    "        \"\"\" Initialize the internal parameter\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class NNModuleParaFree(NNModule):\n",
    "    \"\"\"Specialization of the NNModule for modules which do not have any internal parameters\n",
    "\n",
    "    \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def initialize_parameter(self):\n",
    "        # No initialization necessary\n",
    "        return\n",
    "\n",
    "    def get_grad_param(self, grad_out):\n",
    "        # No parameter gradients\n",
    "        return None\n",
    "\n",
    "    def apply_parameter_update(self, acc_grad_para, up_fun):\n",
    "        # No parameters to update\n",
    "        return\n",
    "\n",
    "\n",
    "class LossModule(NNModule):\n",
    "    \"\"\"Specialization of NNModule for losses which need target values\n",
    "\n",
    "    \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def set_targets(self, t):\n",
    "        \"\"\"Saves expected targets.\n",
    "        Does not copy the input.\n",
    "\n",
    "        :param t: Expected target values.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "\n",
    "    def initialize_parameter(self):\n",
    "        # No internal parameters\n",
    "        return\n",
    "\n",
    "    def get_grad_param(self, grad_out):\n",
    "        # No gradient for internal parameter\n",
    "        return None\n",
    "\n",
    "    def apply_parameter_update(self, acc_grad_para, up_fun):\n",
    "        # No update needed\n",
    "        return\n",
    "\n",
    "\n",
    "# Task 2 a)\n",
    "class Linear(NNModule):\n",
    "    \"\"\"Module which implements a linear layer\"\"\"\n",
    "\n",
    "    #####Start Subtask 2a#####\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # Nbr input neurons\n",
    "        self.n_in = n_in\n",
    "        # Nbr output neurons\n",
    "        self.n_out = n_out\n",
    "        # Weights\n",
    "        self.W = None\n",
    "        # Biases\n",
    "        self.b = None\n",
    "        # Input cache for bprop\n",
    "        self.cache_input = None\n",
    "\n",
    "    def initialize_parameter(self):\n",
    "        # Glorot intialization\n",
    "        sigma = np.sqrt(2.0 / (self.n_in + self.n_out))\n",
    "        self.W = np.random.normal(0, sigma, (self.n_in, self.n_out))\n",
    "        self.b = np.zeros((1, self.n_out))\n",
    "        # self.W = np.zeros((self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, input):\n",
    "        # A copy of the input for deriving parameter update\n",
    "        self.cache_input = np.array(input)\n",
    "        return np.matmul(input, self.W) + self.b\n",
    "\n",
    "    def bprop(self, grad_out):\n",
    "        return np.matmul(grad_out, self.W.transpose())\n",
    "\n",
    "    def get_grad_param(self, grad_out):\n",
    "        grad_w = np.matmul(self.cache_input.transpose(), grad_out)\n",
    "        # Distinguish batch mode\n",
    "        grad_b = np.sum(grad_out, 0) if grad_out.ndim > 1 else grad_out\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def apply_parameter_update(self, acc_grad_para, up_fun):\n",
    "        self.W = up_fun(self.W, acc_grad_para[0])\n",
    "        self.b = up_fun(self.b, acc_grad_para[1])\n",
    "\n",
    "    #####End Subtask#####\n",
    "\n",
    "\n",
    "# Task 2 b)\n",
    "class Softmax(NNModuleParaFree):\n",
    "    \"\"\"Softmax layer\"\"\"\n",
    "\n",
    "    #####Start Subtask 2b#####\n",
    "    def __init__(self):\n",
    "        # Cache output for bprob\n",
    "        self.cache_out = None\n",
    "\n",
    "    def fprop(self, input):\n",
    "        # See 4a for stability reasons\n",
    "        inp_max = np.max(input, 1)\n",
    "        # Transpose -> Numpy subtracts from each batch is inp_max using numpy's broadcasting\n",
    "        exponentials = np.exp((input.transpose() - inp_max).transpose())\n",
    "        normalization = np.sum(exponentials, 1)\n",
    "\n",
    "        # Transpose -> numpy broadcast -> see above\n",
    "        output = (exponentials.transpose() / normalization).transpose()\n",
    "        self.cache_out = np.array(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def bprop(self, grad_out):\n",
    "        if grad_out.ndim == 2:\n",
    "            sz_batch, n_out = grad_out.shape\n",
    "        else:\n",
    "            sz_batch = 1\n",
    "            n_out = len(grad_out)\n",
    "\n",
    "        # 1. term\n",
    "        v_s = np.empty((sz_batch, 1))\n",
    "        for i in range(sz_batch):\n",
    "            v_s[i, :] = np.dot(grad_out[i, :], self.cache_out[i, :])\n",
    "        # 2. term\n",
    "        v_v_s = grad_out - np.broadcast_to(v_s, (sz_batch, n_out))\n",
    "        z = np.multiply(self.cache_out, v_v_s)\n",
    "        return z\n",
    "\n",
    "    #####End Subtask#####\n",
    "\n",
    "\n",
    "# Task 2 c)\n",
    "class CrossEntropyLoss(LossModule):\n",
    "    \"\"\"Cross-Entropy-Loss-Module\"\"\"\n",
    "    def __init__(self):\n",
    "        # Save input for bprop\n",
    "        self.cache_in = None\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.cache_in = np.array(input)\n",
    "        sz_batch = input.shape[0]\n",
    "        loss = -1 * np.log(input[np.arange(sz_batch), self.t])\n",
    "        return loss\n",
    "\n",
    "    def bprop(self, grad_out):\n",
    "        sz_batch, n_in = self.cache_in.shape\n",
    "        z = np.zeros((sz_batch, n_in))\n",
    "        z[np.arange(sz_batch), self.t] =  \\\n",
    "            -1 * 1.0/self.cache_in[np.arange(sz_batch), self.t]\n",
    "        np.multiply(grad_out, z, z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ef6128d-90cf-40d6-9d64-712d5360aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_entire_dataset(path_data, path_label):\n",
    "    logging.info('Reading data...')\n",
    "    data = genfromtxt(path_data, delimiter=' ', dtype=float)\n",
    "    data /= 255.0\n",
    "    labels = genfromtxt(path_label, delimiter=' ', dtype=int)\n",
    "    logging.info('Data read')\n",
    "    return data, labels\n",
    "\n",
    "# Assuming file paths\n",
    "train_images_path = 'mnist-train-data.csv'\n",
    "train_labels_path = 'mnist-train-labels.csv'\n",
    "test_images_path = 'mnist-test-data.csv'\n",
    "test_labels_path = 'mnist-test-labels.csv'\n",
    "valid_images_path = 'mnist-valid-data.csv'\n",
    "valid_labels_path = 'mnist-valid-labels.csv'\n",
    "\n",
    "# Load the data using the read_entire_dataset function\n",
    "train_images, train_labels = read_entire_dataset(train_images_path, train_labels_path)\n",
    "test_images, test_labels = read_entire_dataset(test_images_path, test_labels_path)\n",
    "valid_images, valid_labels = read_entire_dataset(valid_images_path, valid_labels_path)\n",
    "\n",
    "# Optionally, convert Numpy arrays to DataFrames if needed\n",
    "train_images = pd.DataFrame(train_images)\n",
    "train_labels = pd.DataFrame(train_labels)\n",
    "test_images = pd.DataFrame(test_images)\n",
    "test_labels = pd.DataFrame(test_labels)\n",
    "valid_images = pd.DataFrame(valid_images)\n",
    "valid_labels = pd.DataFrame(valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb9f0a6-226a-45f2-a082-5c9cfb46e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7]\n",
      " [2]\n",
      " [1]\n",
      " ...\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "valid_images = np.array(valid_images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ca859fd-4a99-46fc-9a67-af78a6126430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(inputs, labels, valid_inputs, valid_labels, epochs=100, batch_size=600, learning_rate=0.01):\n",
    "    # Initialize the network architecture\n",
    "    net = [Linear(28*28, 10), Softmax()]\n",
    "    loss = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training dataset\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "        inputs = inputs[indices]\n",
    "        labels = labels[indices]\n",
    "\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # Training Phase\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            z = batch_inputs\n",
    "            for module in net:\n",
    "                z = module.fprop(z)\n",
    "            loss.set_targets(batch_labels)\n",
    "            E = loss.fprop(z)\n",
    "\n",
    "            # Backward pass\n",
    "            dz = loss.bprop(1.0 / batch_size)  # Normalize the gradient by the batch size\n",
    "            for module in reversed(net):\n",
    "                dz = module.bprop(dz)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for module in net:\n",
    "                if isinstance(module, Linear):\n",
    "                    module.apply_parameter_update(dz, learning_rate)\n",
    "\n",
    "            total_loss += np.sum(E)\n",
    "            predictions = np.argmax(z, axis=1)\n",
    "            correct_predictions += np.sum(predictions == batch_labels)\n",
    "\n",
    "        avg_loss = total_loss / len(inputs)\n",
    "        accuracy = correct_predictions / len(inputs) * 100\n",
    "        print(f\"Training - Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Validation Phase\n",
    "        total_val_loss = 0\n",
    "        correct_val_predictions = 0\n",
    "        for i in range(0, len(valid_inputs), batch_size):\n",
    "            batch_inputs = valid_inputs[i:i + batch_size]\n",
    "            batch_labels = valid_labels[i:i + batch_size]\n",
    "\n",
    "            # Forward pass only\n",
    "            z = batch_inputs\n",
    "            for module in net:\n",
    "                z = module.fprop(z)\n",
    "            loss.set_targets(batch_labels)\n",
    "            E = loss.fprop(z)\n",
    "\n",
    "            total_val_loss += np.sum(E)\n",
    "            predictions = np.argmax(z, axis=1)\n",
    "            correct_val_predictions += np.sum(predictions == batch_labels)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(valid_inputs)\n",
    "        val_accuracy = correct_val_predictions / len(valid_inputs) * 100\n",
    "        print(f\"Validation - Epoch {epoch + 1}/{epochs}, Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Assuming inputs, labels, valid_inputs, and valid_labels are available and properly preprocessed\n",
    "# train_network(inputs, labels, valid_inputs, valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ebc6d7d-b27f-42bd-beef-432b9f76669f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113/3714443447.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_113/1082679157.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(inputs, labels, valid_inputs, valid_labels, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_113/2075550390.py\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# A copy of the input for deriving parameter update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "train_network(train_images,train_labels,valid_images,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd4a1e-8872-4241-b790-c9da61e2a2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
